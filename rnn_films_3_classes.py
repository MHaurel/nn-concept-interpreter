# -*- coding: utf-8 -*-
"""RNN_films_3_classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KR9nDKuHNBQxBeSsMNErDZ_HQBr4qTzb

# Building a neural network to predict income from description of a film

Establish connection to drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### SPARQL query to fetch title, description, income of films from dbpedia


SPARQL query to fetch data
```sparql
select distinct ?film ?income ?cat ?desc where {
?film a <http://dbpedia.org/ontology/Film> .
?film <http://dbpedia.org/ontology/gross> ?income .
?film <http://dbpedia.org/ontology/abstract> ?desc .
  {
    ?film <http://purl.org/dc/terms/subject> ?cat . 
    } UNION {
      ?film <http://purl.org/dc/terms/subject> ?scat . 
      ?scat <http://www.w3.org/2004/02/skos/core#broader> ?cat.
  } 
  filter (lang(?desc) = "en")
  filter (lang(?film) = "en")
} group by ?film ?cat ?desc LIMIT 3 OFFSET 100000
```
After doing some requests, I found out that sparql endpoint from dbpedia allows us to get only 10,000 rows at a same time. So, with OFFSET and LIMIT, I managed to get more than 10K entries and I have now a dataset containing 50K rows

## Data understanding
"""

import pandas as pd
import numpy as np

import glob as glob

df = pd.DataFrame()


# filepath = ".\data\\" # Local notebook in a data subfolder
# filepath = '.\\' # If using temp files on colab
filepath = '/content/drive/MyDrive/Cours/Stage/data/' # My drive repo

for filename in glob.glob(filepath + 'data-*.csv'):
    print(f"Concatening {filename} to df...")
    df = pd.concat([df, pd.read_csv(filename)], axis=0)

df.shape

df.head()

df.tail()

df.info()

df.describe()

df.columns

df = df.drop_duplicates()
df.shape

"""df = pd.concat([df.groupby("film").mean(), df.groupby('film').nth(0).film, df.groupby('film').nth(0).cat, df.groupby("film").nth(0).desc], axis=1)
df.head()

Creating another dataset to map each film to its related categories
"""

df_cat = pd.concat([df.film, df.cat], axis=1)
df_cat.head()

"""Normalizing categories to not have the full link"""

def normalize_cat(x):
    return "".join(x.split(':')[2])

df_cat['cat_p'] = df_cat['cat'].apply(lambda x: normalize_cat(x))

df_cat.head()

"""Getting unique films, mean of all the incomes related and unique descriptions"""

films = df.film.unique()

incomes = []
for f in films:
    incomes.append(df[df.film == f].income.mean())

df_film = pd.DataFrame(films)
df_desc = pd.DataFrame(df.desc.unique())
df_income = pd.DataFrame(incomes)

df_new = pd.DataFrame()
df_new['film'] = df_film
df_new['income'] = df_income
df_new['desc'] = df_desc

df_new.head()

df = df_new
df.shape

"""Removing outliers"""

zscore = (df.income - df.income.mean()) / df.income.std()
dfwo = df[abs(zscore)<2.0]


dfwo=df[df.income < 500000000]
dfwo.describe()

dfwo.shape

"""Visualizing distribution through violinplot"""

import plotly.express as px

fig = px.violin(dfwo, y="income")
fig.show()

MEDIAN_VALUE = dfwo[dfwo.income < 10000000].income.median()
MEDIAN_VALUE

import matplotlib.pyplot as plt

def plot_range(ar, marker='|'):
  f = plt.figure()
  f.set_figwidth(20)
  f.set_figheight(3)
  val = 0. 
  plt.plot(ar, np.zeros_like(ar) + val, 'x', marker=marker)
  plt.show()

plot_range(np.array(dfwo.income))

"""## Preprocessing

### Overall data
"""

dfwo.isnull().sum()

"""Creating 3 classes for our NN to classify:
- [< MEDIAN-VALUE] : Medium-Low
- [MEDIAN-VALUE - 10e6] : Medium-High
- [> 10e6] : Exceptional
"""

dfwo.loc[(dfwo.income >= 0), 'income_c'] = 'medium-low'
dfwo.loc[(dfwo.income >= MEDIAN_VALUE), 'income_c'] = 'medium-high'
dfwo.loc[(dfwo.income >= 10e6), 'income_c'] = 'exceptional'
dfwo.head()

dfwo.info()

"""### Text description

Function removing stopwords, links, digits, ...
"""

import string
import re
import nltk
nltk.download('stopwords')

stopwords = nltk.corpus.stopwords.words('english')

def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  
  # Remove links starting with http
  text1 = re.sub(r'http\S+', ' ', text)

  # Remove digits
  text2 = re.sub(r'\d+', ' ', text1)
  tokens = re.split('\W+', text2)
  text = [word for word in tokens if word not in stopwords+[""]] # MdA: Added the empty string to stopwords

  return text

"""Applying the function to all the DataFrame"""

dfwo['desc_p'] = dfwo['desc'].apply(lambda x: clean_text(x))
# MdA: created a new column, as otherwise when applied twice it 
# messes things up
dfwo.head()

X = dfwo['desc_p'] # MdA: use the new column
y = dfwo['income_c']

"""### One-hot encoding on target column"""

y = pd.get_dummies(y)
y.head()

"""### Splitting train & test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

pd.DataFrame(X_test).desc_p.iloc[0]

print(X_train.shape)
print(X_test.shape)

print(y_train.shape)
print(y_test.shape)

"""### Tokenizing text"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# The maximum number of words to be used (most frequent)
MAX_NB_WORDS = 10000

# Max number of words in each Tweet
MAX_SEQUENCE_LENGTH = 100

# Intialize and fit the tokenizer
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True, split=' ')
tokenizer.fit_on_texts(X_train)

"""Transforming into numerical sequences"""

# Use that tokenizer to transform the text messages in the training and test sets
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

print(X_train[10])
print(X_train_seq[10])

"""Get max length of a sequence in the DataFrame and the value of the mean + the standard deviation"""

from statistics import *

def get_max_len(seq):
    max_len = 0
    for x in seq:
        if len(x) > max_len:
            max_len = len(x)
    return max_len

def get_mean_std_len(seq):
    values = []
    for x in seq:
        values.append(len(x))
    
    std = stdev(values)
    m = mean(values)
    return int(m + std)

print(f"Max len: {get_max_len(X_train_seq)}")
print(f"Mean + std: {get_mean_std_len(X_train_seq)}")

"""Padding the sequence so each sequence has the same length"""

# X_train_seq_padded = pad_sequences(X_train_seq, 100)
# X_train_seq_padded = pad_sequences(X_train_seq, 44)
# X_train_seq_padded = pad_sequences(X_train_seq, get_max_len(X_train_seq))
X_train_seq_padded = pad_sequences(X_train_seq, get_mean_std_len(X_train_seq))

# X_test_seq_padded = pad_sequences(X_test_seq, 100)
# X_test_seq_padded = pad_sequences(X_test_seq, 44)
# X_test_seq_padded = pad_sequences(X_test_seq, get_max_len(X_train_seq))
X_test_seq_padded = pad_sequences(X_test_seq, get_mean_std_len(X_train_seq))

X_train_seq_padded[10]

print('X_train_seq_padded:', X_train_seq_padded.shape)
print('X_test_seq_padded:', X_test_seq_padded.shape)

print('y_train:', y_train.shape)
print('y_test:', y_test.shape)

"""## Model Building"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, SpatialDropout1D
from tensorflow.keras.optimizers import Adam, RMSprop, Adadelta, Adagrad
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=64, input_length=X_train_seq_padded.shape[1]))

# model.add(SpatialDropout1D(0.4))

# Need to activate with tanh and set recurrent_dropout to 0 for the GPU to process
# Initially activation='relu' & recurrent_dropout=0.2
model.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, return_sequences=True, name="lstm_1"))
model.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, return_sequences=True, name="lstm_2"))
model.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, name="lstm_3"))

model.add(Dense(64, activation='relu')) 

# model.add(Dropout(0.4))

model.add(Dense(3, activation='softmax')) 


model.summary()

"""optimizers : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
- https://deepdatascience.wordpress.com/2016/11/18/which-lstm-optimizer-to-use/
-https://deepdatascience.files.wordpress.com/2016/11/contours_evaluation_optimizers.gif?w=616
- https://deepdatascience.files.wordpress.com/2016/11/saddle_point_evaluation_optimizers.gif?w=616

loss fns : https://www.tensorflow.org/api_docs/python/tf/keras/losses

metrics : https://www.tensorflow.org/api_docs/python/tf/keras/metrics

Compile the model
"""

# model.compile(optimizer=Adam(learning_rate=0.0001, clipnorm=0.001, clipvalue=0.5), loss='categorical_crossentropy', metrics=['accuracy'])
model.compile(optimizer=RMSprop(learning_rate=0.0001, clipnorm=0.001, clipvalue=0.5), loss='categorical_crossentropy', metrics=['accuracy'])

# Adding an early stopping
es = EarlyStopping(monitor='val_accuracy', 
                   mode='max', 
                   patience=3, #Stop the model training if the validation accuracy doesnt increase in 3 consecutive Epochs
                   restore_best_weights=True)

BATCH_SIZE = 64
EPOCHS = 30

"""Fit the RNN"""

# For the GPU training
with tf.device('/device:GPU:0'):
    history = model.fit(X_train_seq_padded, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_test_seq_padded, y_test))
  
# history = model.fit(X_train_seq_padded, y_train, batch_size=BATCH_SIZE, callbacks=[es], 
#                     epochs=EPOCHS, validation_data=(X_test_seq_padded, y_test))

"""(optional) Save the model"""

model.save('./rnn-3')

"""Might be overfitting https://datascience.stackexchange.com/questions/97132/my-lstm-has-a-really-low-accuracy-is-there-anyway-to-improve-it

Ways to reduce overfitting:
- use more data
- Add dropout layer
- reduce the number of neurons at each layer of the model
- use a different metric rather than **accuracy**
- use **batch normalization** instead of **Dropout**

Other theory is : **Exploding gradient** and could be resolved with **gradient clipping**

https://machinelearningmastery.com/exploding-gradients-in-neural-networks/

So, maybe use a smaller batch size ??

"In the Keras deep learning library, you can use gradient clipping by setting the clipnorm or clipvalue arguments on your optimizer before training. Good default values are **clipnorm=1.0** and **clipvalue=0.5**."

## Model Evaluation

#### Plotting model performance
Get values of accuracy, val_accuracy, loss and val_loss to plot them
"""

import plotly.express as px

def plot_acc_loss():
  
  h = history.history

  df_acc = pd.DataFrame()
  df_acc["acc"] = h['accuracy']
  df_acc["val_acc"] = h['val_accuracy']

  df_loss = pd.DataFrame()

  df_loss["loss"] = h['loss']
  df_loss["val_loss"] = h['val_loss']

  fig = px.line(df_acc, title='Accuracy of the model', labels = {'index': "Epoch"}, width=800, height=400)
  fig.show()

  fig = px.line(df_loss, title='Loss of the model', labels = {'index': "Epoch"}, width=800, height=400)
  fig.show()

plot_acc_loss()

score = model.evaluate(X_test_seq_padded, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

y_test

"""#### Confusion Matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8,8))

y_pred = model.predict(X_test_seq_padded) # MdA: get the predictions for X_test

# MdA: show the confusion matrix
y_pred_r = np.argmax(y_pred, axis=1)
y_test_r = np.argmax(np.array(y_test), axis=1)
matrix = confusion_matrix(y_test_r,y_pred_r)
disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=["exceptional","medium-high","medium-low"])
disp.plot(ax=ax)

model.optimizer.learning_rate, model.optimizer.clipnorm, model.optimizer.clipvalue

"""## To load the model"""

from tensorflow import keras
model_loaded = keras.models.load_model("/content/drive/MyDrive/Cours/Stage/models/rnn-3")
# model_loaded = keras.models.load_model("./rnn-3")
model_loaded

"""**ERASE THE PREVIOUS MODEL**"""

model = model_loaded

"""# Visualize and understand neuron activations"""

list(map(lambda x: x.name, model.layers))

"""### Re-creating second model 
Re-creating second model identical from the one trained but removing the last dense layer to collect activations values
"""

model.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout

model2 = Sequential()
model2.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=64, input_length=X_train_seq_padded.shape[1], weights=model.layers[0].get_weights()))

activations_embedding = model2.predict(X_test_seq_padded)
activations_embedding.shape

for i, layer in enumerate(model.layers):
  print(i, layer.name)

model2 = Sequential()
model2.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=64, input_length=X_train_seq_padded.shape[1], weights=model.layers[0].get_weights()))

model2.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, return_sequences=True, name="lstm_1", weights=model.layers[1].get_weights()))
model2.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, return_sequences=True, name="lstm_2", weights=model.layers[2].get_weights()))
model2.add(LSTM(128, activation='tanh', dropout=0.2, recurrent_dropout=0, name="lstm_3", weights=model.layers[3].get_weights()))

model2.add(Dense(64, activation='relu', weights=model.layers[4].get_weights()))

model2.summary()

activations = model2.predict(X_test_seq_padded)
activations

activations.T.shape

activations.shape

pd.DataFrame(activations).describe()

activations[0].shape

"""Creating DataFrame containing neurons and activations values for each neuron"""

def get_activation_matrix(activations):
    df_dict = {
        'value': [],
        'neuron_index': []
    }

    # MH: transpose the matrix to have neurons as index
    for neuron_index, value_list in enumerate(activations.T):
      # print(neuron_index, value_list)
      for value in value_list:
        df_dict['value'].append(value)
        df_dict['neuron_index'].append(neuron_index + 1) # MH: Should we let first neuron at 0 ?
      # print(neuron_index)

    df_act = pd.DataFrame().from_dict(df_dict)
    return df_act

def get_activation_matrix_per_neuron(activations):
    df = pd.DataFrame()

    # MH: transpose the matrix to have neurons as index
    for neuron_index, value_list in enumerate(activations.T):
      # print(neuron_index, value_list)
      index = f"neuron_{neuron_index + 1}" # Should we let 1st neuron at 0 ?
      df[index] = value_list

    return df

df_act_matrix = get_activation_matrix_per_neuron(activations)
df_act_matrix

# We get all the films for a given category
def get_films_from_cat(cat):
  return df_cat[df_cat.cat_p == cat]

# We search for the film associated to a sequence of preprocessed words
def get_film_from_seq(seq):
    for i in range(len(dfwo.desc_p)):
        if dfwo.desc_p.iloc[i] == seq:
            return dfwo.iloc[i] # added [0]
    return pd.DataFrame()

# Then we get categories from the film uri
def get_cats_from_film(film):
    cats= []
    fetch_cats = df_cat[df_cat.film == film].cat_p.unique()
    for cat in fetch_cats:
        cats.append(cat)
        
    return cats

# Upper function which gets categories for a given sequence
def get_cats_from_seq(seq):
    return get_cats_from_film(get_film_from_seq(seq).film)

def get_film_index(film):
  for i in range(len(X_test)): # X_test
    if X_test.iloc[i] == film.desc_p:
      return i
  return None

# Get the list of distinct categories
def get_unique_cats():
    cat_length = []
    for cat in df_cat.cat_p.unique():
        cat_length.append((cat, df_cat[df_cat.cat_p == cat].shape[0]))
    return cat_length

# Get categories which number of films associated is greater (or equal) than
# a given value
def get_cats_greater_or_equal_than(value):
    cats = []
    for l in get_unique_cats():
        cat, n = l
        if n >= value:
            cats.append((cat, n))
    return cats

activations_row = df.iloc[0, 1:]
for j in range(len(activations_row)):
  print(activations_row[j])

# Return the list of activations (list) related to a given category
def get_activations_for_cat(category):
    activations = []
    for i in range(len(df.categories)):
        if category in df.categories.iloc[i]:
#           print(df_cat_act.cat.iloc[i], i)
            act = []
            activations_row = df.iloc[i, 1:]
            for j in range(len(activations_row)):
              act.append(activations_row[j])
            activations.append(act)
    return np.array(activations)

def get_activations_for_seq(model, seq, full_seq, X_test):
  activations = model.predict(full_seq)
  for i in range(3): #(len(X_test)):
    print(pd.DataFrame(X_test).desc_p.iloc[i], seq)
    # if pd.DataFrame(X_test).desc_p == seq:
    #   index = i
    #   return activations[i]
  # return None # pd.DataFrame()


# get_activations_for_seq(model2, X_test_seq_padded[0], X_test_seq_padded, X_test)
def get_seq_padded_index(seq):
  for i in range(len(X_test_seq_padded)):
    if X_test_seq_padded[i].any() == seq.any():
      return i, X_test_seq_padded, seq
  return None

dict_cat_all = {
    'categories': []
}

for i in range(len(X_test)):
  dict_cat_all['categories'].append(get_cats_from_seq(X_test.iloc[i]))

df_cat_all = pd.DataFrame().from_dict(dict_cat_all)

df = pd.concat([df_cat_all, df_act_matrix], axis=1)
df.head()

"""## Store activations
Store activations into a csv file to re-use it in a different notebook for cleaner code
"""

import os

df.to_csv("/content/drive/MyDrive/Cours/Stage/data/activations.csv", index=False)

"""### Visualizing activation values over each neuron"""

def plot_activations(activations, title="Activation values"):
    fig = px.scatter(activations, x='neuron_index', y='value', 
           title=title, labels={'neuron_index': 'Neuron', 'value': 'Value'})
    fig.show()

"""# Loading categories"""

df_cat.head()

df_cat.describe()

df_cat.shape

"""Number of distinct categories we have"""

df_cat.cat.unique().shape

"""## Search for films and categories related to a given activation"""

activations[0]

film = get_film_from_seq(pd.DataFrame(X_test).desc_p.iloc[578])
get_film_index(film)

cats = get_cats_from_seq(X_test.iloc[0])
cats

"""Creating dict of activations associated to categories

Sample use of getting activations for a given category
"""

get_activations_for_cat('Metro-Goldwyn-Mayer_films')

"""Then plot activations for this category"""

plot_activations(get_activation_matrix(get_activations_for_cat('Metro-Goldwyn-Mayer_films')), 
                 f"Activation values for Metro-Goldwyn-Mayer_films")

"""Getting most popular categories (n >= 200)"""

popular_categories = get_cats_greater_or_equal_than(200)
popular_categories

""" ## Plot activations for all the most popular categories"""

def plot_activation_for_categories(categories):
    for cat in categories:
        _cat = cat[0]
        df_sample = get_activation_matrix(get_activations_for_cat(_cat))
        plot_activations(df_sample, f"Activations values for category: {_cat}")

df_test = get_activation_matrix(get_activations_for_cat('Metro-Goldwyn-Mayer_films'))
df_test

def get_mean_activation_matrix(category):
  means = []
  _df_cat = get_activation_matrix(get_activations_for_cat(category))
  for i in range(1, 64 + 1):
    means.append(_df_cat[_df_cat.neuron_index == i].value.mean())

  return means

def plot_mean_activations_for_category(categories):
  for cat in categories:
        _cat = cat[0]
        df_sample = get_mean_activation_matrix(_cat)
        fig = px.scatter(df_sample, title=f"Mean activation values for {_cat}")
        fig.show()

means = []
for i in range(1, 64 + 1):
  means.append(df_test[df_test.neuron_index == i].value.mean())
  # print(i, ":", df_test[df_test.neuron_index == i].value.mean())

px.scatter(means, title="Mean activation per each neuron for category 'Metro-Goldwyn-Mayer_films'")

len(get_activations_for_cat('Metro-Goldwyn-Mayer_films'))

get_activation_matrix_per_neuron(get_activations_for_cat('Metro-Goldwyn-Mayer_films'))

"""We can have a boxplot on activations for this category"""

px.box(get_activation_matrix_per_neuron(get_activations_for_cat('Metro-Goldwyn-Mayer_films')))

pd.read_csv('/content/drive/MyDrive/Cours/Stage/data/activations.csv').shape

seq = X_test.iloc[0]
get_film_from_seq(seq).film

df_cat

